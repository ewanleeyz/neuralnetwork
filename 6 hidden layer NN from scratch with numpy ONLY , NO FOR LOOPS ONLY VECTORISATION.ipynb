{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f6809677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.special import softmax as sm\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, num_features, num_hidden1,num_hidden2,num_hidden3,num_hidden4,num_hidden5,num_hidden6 ,alpha, max_epochs, num_output, _EPSILON):\n",
    "        super().__init__()\n",
    "        self.num_features=num_features  # number of input nodes (features)\n",
    "        self.num_hidden1=num_hidden1  # number of hidden nodes for 1st hidden layer\n",
    "        self.num_hidden2=num_hidden2  # number of hidden nodes for 2nd hidden layer\n",
    "        self.num_hidden3=num_hidden3  # number of hidden nodes for 3rd hidden layer\n",
    "        self.num_hidden4=num_hidden4  # number of hidden nodes for 4th hidden layer\n",
    "        self.num_hidden5=num_hidden5  # number of hidden nodes for 5th hidden layer\n",
    "        self.num_hidden6=num_hidden6  # number of hidden nodes for 6th hidden layer\n",
    "        self.alpha=alpha  # learning rate\n",
    "        self.max_epochs=max_epochs # maximum number of epochs\n",
    "        self.num_output=num_output # number of output nodes\n",
    "        self._EPSILON=_EPSILON\n",
    "        self.loss = [] #list to store losses per 100 epochs \n",
    "        self.Weights_Input_to_H1=np.random.randn(self.num_hidden1, self.num_features)*(2/sqrt(self.num_features))\n",
    "        self.Bias_Input_to_H1=np.zeros([self.num_hidden1,1])\n",
    "        self.Weights_H1_to_H2=np.random.randn(self.num_hidden2, self.num_hidden1)*(2/sqrt(self.num_hidden1))\n",
    "        self.Bias_H1_to_H2=np.zeros([self.num_hidden2,1])\n",
    "        self.Weights_H2_to_H3=np.random.randn(self.num_hidden3, self.num_hidden2)*(2/sqrt(self.num_hidden2))\n",
    "        self.Bias_H2_to_H3=np.zeros([self.num_hidden3,1])\n",
    "        self.Weights_H3_to_H4=np.random.randn(self.num_hidden4, self.num_hidden3)*(2/sqrt(self.num_hidden3))\n",
    "        self.Bias_H3_to_H4=np.zeros([self.num_hidden4,1])\n",
    "        self.Weights_H4_to_H5=np.random.randn(self.num_hidden5, self.num_hidden4)*(2/sqrt(self.num_hidden4))\n",
    "        self.Bias_H4_to_H5=np.zeros([self.num_hidden5,1])\n",
    "        self.Weights_H5_to_H6=np.random.randn(self.num_hidden6, self.num_hidden5)*(2/sqrt(self.num_hidden5))\n",
    "        self.Bias_H5_to_H6=np.zeros([self.num_hidden6,1])\n",
    "        self.Weights_H6_to_output=np.random.randn(self.num_output, self.num_hidden6)*(2/sqrt(self.num_hidden6))\n",
    "        self.Bias_H6_to_output=np.zeros([self.num_output,1])\n",
    "        self.dWeights_Input_to_H1=np.zeros([self.num_hidden1, self.num_features])\n",
    "        self.dBias_Input_to_H1=np.zeros([self.num_hidden1,1])\n",
    "        self.dWeights_H1_to_H2=np.zeros([self.num_hidden2, self.num_hidden1])\n",
    "        self.dBias_H1_to_H2=np.zeros([self.num_hidden2,1])\n",
    "        self.dWeights_H2_to_H3=np.zeros([self.num_hidden3, self.num_hidden2])\n",
    "        self.dBias_H2_to_H3=np.zeros([self.num_hidden3,1])\n",
    "        self.dWeights_H3_to_H4=np.zeros([self.num_hidden4, self.num_hidden3])\n",
    "        self.dBias_H3_to_H4=np.zeros([self.num_hidden4,1])\n",
    "        self.dWeights_H4_to_H5=np.zeros([self.num_hidden5, self.num_hidden4])\n",
    "        self.dBias_H4_to_H5=np.zeros([self.num_hidden5,1])\n",
    "        self.dWeights_H5_to_H6=np.zeros([self.num_hidden6, self.num_hidden5])\n",
    "        self.dBias_H5_to_H6=np.zeros([self.num_hidden6,1])\n",
    "        self.dWeights_H6_to_output=np.zeros([self.num_output, self.num_hidden6])\n",
    "        self.dBias_H6_to_output=np.zeros([self.num_output,1])\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def relU(self,X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    def deriv(self,X):\n",
    "        X[X>=0]=1\n",
    "        X[X<0]=0\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Softmax_grad(self,X):\n",
    "        return(sm(X,axis=1)*(1-sm(X,axis=1)))\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e=np.exp(x)\n",
    "        for i in range(e.shape[1]):\n",
    "            e[:,i]=e[:,i]/np.sum(e[:,i])\n",
    "        return e\n",
    "    \n",
    "    def derivtanh(self,X):\n",
    "        return 1-((np.tanh(X))**2)\n",
    "    \n",
    "    def categorical_cross_entropy(self,actual, predicted):\n",
    "        sum_score = 0.0\n",
    "        for i in range(len(actual)):\n",
    "            for j in range(len(actual[i])):\n",
    "                sum_score += actual[i][j] * log(self._EPSILON + predicted[i][j])\n",
    "        mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "        return -mean_sum_score\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    # TODO: complete implementation for forward pass\n",
    "    def forward(self, X):\n",
    "        self.z1=np.dot((self.Weights_Input_to_H1),(X))+self.Bias_Input_to_H1\n",
    "        self.a1=self.relU(self.z1)\n",
    "        self.z2=np.dot((self.Weights_H1_to_H2),(self.a1))+self.Bias_H1_to_H2\n",
    "        self.a2=self.relU(self.z2)\n",
    "        self.z3=np.dot((self.Weights_H2_to_H3),(self.a2))+self.Bias_H2_to_H3\n",
    "        self.a3=self.relU(self.z3)\n",
    "        self.z4=np.dot((self.Weights_H3_to_H4),(self.a3))+self.Bias_H3_to_H4\n",
    "        self.a4=self.relU(self.z4)\n",
    "        self.z5=np.dot((self.Weights_H4_to_H5),(self.a4))+self.Bias_H4_to_H5\n",
    "        self.a5=self.relU(self.z5)\n",
    "        self.z6=np.dot((self.Weights_H5_to_H6),(self.a5))+self.Bias_H5_to_H6\n",
    "        self.a6=self.relU(self.z6)\n",
    "        self.z7=np.dot((self.Weights_H6_to_output),(self.a6))+self.Bias_H6_to_output\n",
    "        self.a7=self.softmax((self.z7))\n",
    "        return self.a7\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # TODO: complete implementation for backpropagation\n",
    "    # the following Numpy functions may be useful: np.dot, np.sum, np.tanh, numpy.ndarray.T\n",
    "    def backprop(self, X, t):\n",
    "            \n",
    "        self.dWeights_Input_to_H1=np.zeros([self.num_hidden1, self.num_features])\n",
    "        self.dBias_Input_to_H1=np.zeros([self.num_hidden1,1])\n",
    "        self.dWeights_H1_to_H2=np.zeros([self.num_hidden2, self.num_hidden1])\n",
    "        self.dBias_H1_to_H2=np.zeros([self.num_hidden2,1])\n",
    "        self.dWeights_H2_to_H3=np.zeros([self.num_hidden3, self.num_hidden2])\n",
    "        self.dBias_H2_to_H3=np.zeros([self.num_hidden3,1])\n",
    "        self.dWeights_H3_to_H4=np.zeros([self.num_hidden4, self.num_hidden3])\n",
    "        self.dBias_H3_to_H4=np.zeros([self.num_hidden4,1])\n",
    "        self.dWeights_H4_to_H5=np.zeros([self.num_hidden5, self.num_hidden4])\n",
    "        self.dBias_H4_to_H5=np.zeros([self.num_hidden5,1])\n",
    "        self.dWeights_H5_to_H6=np.zeros([self.num_hidden6, self.num_hidden5])\n",
    "        self.dBias_H5_to_H6=np.zeros([self.num_hidden6,1])\n",
    "        self.dWeights_H6_to_output=np.zeros([self.num_output, self.num_hidden6])\n",
    "        self.dBias_H6_to_output=np.zeros([self.num_output,1])\n",
    "        self.dz7=(self.a7.reshape(self.num_output,-1)-t.reshape(self.num_output,-1))\n",
    "        self.dBias_H6_to_output=np.sum(self.dz7,axis=1,keepdims=True)/(X.shape[1])\n",
    "        self.dWeights_H6_to_output=np.dot((self.dz7),self.a6.T)/(X.shape[1])\n",
    "        self.dz6=(np.dot(self.Weights_H6_to_output.T,self.dz7)) * (self.deriv(self.z6))\n",
    "        self.dBias_H5_to_H6=np.sum(self.dz6,axis=1,keepdims=True)/(X.shape[1])\n",
    "        self.dWeights_H5_to_H6=np.dot((self.dz6),(self.a5.T))/(X.shape[1])\n",
    "        self.dz5=(np.dot(self.Weights_H5_to_H6.T,self.dz6)) * (self.deriv(self.z5))\n",
    "        self.dBias_H4_to_H5=np.sum(self.dz5,axis=1,keepdims=True)/(X.shape[1])\n",
    "        self.dWeights_H4_to_H5=np.dot((self.dz5),(self.a4.T))/(X.shape[1])\n",
    "        self.dz4=(np.dot(self.Weights_H4_to_H5.T,self.dz5)) * (self.deriv(self.z4))\n",
    "        self.dBias_H3_to_H4=np.sum(self.dz4,axis=1,keepdims=True)/(X.shape[1])\n",
    "        self.dWeights_H3_to_H4=np.dot((self.dz4),(self.a3.T))/(X.shape[1])\n",
    "        self.dz3=(np.dot(self.Weights_H3_to_H4.T,self.dz4)) * (self.deriv(self.z3))\n",
    "        self.dBias_H2_to_H3=np.sum(self.dz3,axis=1,keepdims=True)/(X.shape[1])\n",
    "        self.dWeights_H2_to_H3=np.dot((self.dz3),(self.a2.T))/(X.shape[1])\n",
    "        self.dz2=(np.dot(self.Weights_H2_to_H3.T,self.dz3)) * (self.deriv(self.z2))\n",
    "        self.dBias_H1_to_H2=np.sum(self.dz2,axis=1,keepdims=True)/(X.shape[1])\n",
    "        self.dWeights_H1_to_H2=np.dot((self.dz2),(self.a1.T))/(X.shape[1])\n",
    "        self.dz1=(np.dot(self.Weights_H1_to_H2.T,self.dz2)) * (self.deriv(self.z1))\n",
    "        self.dBias_Input_to_H1=np.sum(self.dz1,axis=1,keepdims=True)/(X.shape[1])\n",
    "        self.dWeights_Input_to_H1=np.dot((self.dz1),X.T)/(X.shape[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "              \n",
    "                        \n",
    "                \n",
    "      \n",
    "        \n",
    "        \n",
    "    \n",
    "    #TODO: complete implementation for fitting data, and change the existing code if needed\n",
    "    def fit(self, x_train_data, y_train_data,x_dev_data,y_dev_data):\n",
    "       \n",
    "        \n",
    "        \n",
    "        for step in range(self.max_epochs):\n",
    "            self.forward(x_train_data)\n",
    "            self.backprop(x_train_data, y_train_data)\n",
    "            #self.MSE=np.mean((self.dz7/2)**2)\n",
    "            self.CCloss=self.categorical_cross_entropy(y_train_data,self.a7)\n",
    "            #self.CCloss=np.mean(log_loss(y_true=y_train_data,y_pred=self.a7,normalize=True))\n",
    "            self.Bias_H6_to_output=self.Bias_H6_to_output-((self.alpha)*(self.dBias_H6_to_output))\n",
    "            self.Weights_H6_to_output=self.Weights_H6_to_output-((self.alpha)*(self.dWeights_H6_to_output))\n",
    "            self.Bias_H5_to_H6=self.Bias_H5_to_H6-((self.alpha)*(self.dBias_H5_to_H6))\n",
    "            self.Weights_H5_to_H6=self.Weights_H5_to_H6-((self.alpha)*(self.dWeights_H5_to_H6))\n",
    "            self.Bias_H4_to_H5=self.Bias_H4_to_H5-((self.alpha)*(self.dBias_H4_to_H5))\n",
    "            self.Weights_H4_to_H5=self.Weights_H4_to_H5-((self.alpha)*(self.dWeights_H4_to_H5))\n",
    "            self.Bias_H3_to_H4=self.Bias_H3_to_H4-((self.alpha)*(self.dBias_H3_to_H4))\n",
    "            self.Weights_H3_to_H4=self.Weights_H3_to_H4-((self.alpha)*(self.dWeights_H3_to_H4))\n",
    "            self.Bias_H2_to_H3=self.Bias_H2_to_H3-((self.alpha)*(self.dBias_H2_to_H3))\n",
    "            self.Weights_H2_to_H3=self.Weights_H2_to_H3-((self.alpha)*(self.dWeights_H2_to_H3))\n",
    "            self.Bias_H1_to_H2=self.Bias_H1_to_H2-((self.alpha)*(self.dBias_H1_to_H2))\n",
    "            self.Weights_H1_to_H2=self.Weights_H1_to_H2-((self.alpha)*(self.dWeights_H1_to_H2))\n",
    "            self.Bias_Input_to_H1=self.Bias_Input_to_H1-((self.alpha)*(self.dBias_Input_to_H1))\n",
    "            self.Weights_Input_to_H1=self.Weights_Input_to_H1-((self.alpha)*(self.dWeights_Input_to_H1))\n",
    "\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f'step: {step},  loss: {self.CCloss:3.150f}') \n",
    "                print(accuracy_score(np.argmax(y_train_data,axis=0),np.argmax(self.a7,axis=0)))\n",
    "                print(accuracy_score(np.argmax(y_dev_data,axis=0),np.argmax(self.forward(x_dev_data),axis=0)))\n",
    "                self.loss.append(self.CCloss)\n",
    "                \n",
    "           \n",
    "        \n",
    "\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    def predict(self,X,y=None):\n",
    "        self.forward(X)\n",
    "        if(self.num_output>1):\n",
    "            self.a7=np.argmax(self.a7, axis=1)\n",
    "        lin_out1=self.a7\n",
    "        #y_hat=np.where(lin_out1>0.5,1,0)\n",
    "        \n",
    "            \n",
    "        \n",
    "        return lin_out1\n",
    "            \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e56de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "#X=X[:, :]\n",
    "#y=y[:]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae78e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9 3.1 5.1 2.3]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [6.4 3.1 5.5 1.8]]\n",
      "[2 0 2 0 0 2 1 2 1 1 0 1 1 2 0 0 2 1 2 0 1 2 1 1 0 2 0 2 2 0 2 0 1 0 2 1 1\n",
      " 0 1 0 1 1 2 2 0 0 0 2 1 1 1 0 0 0 1 2 1 0 0 2 2 2 2 1 2 1 1 0 2 2 0 1 2 0\n",
      " 2 1 0 0 1 1 2 2 1 1 2 2 1 1 0 1 2 2 2 2 2 2 0 0 1 0 0 2 1 1 2 0 0 0 1 1 2\n",
      " 1 2 1 1 0 0 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X_train, Y_train, test_size=0.11)\n",
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f963c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_original = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\",\n",
    "    header=None)\n",
    "dataset = df_original.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346223a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_original = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\",\n",
    "    header=None)\n",
    "dataset = df_original.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "Y = dataset[:, 60]\n",
    "# convert string labels to binary\n",
    "Y[Y == 'R'] = 0\n",
    "Y[Y == 'M'] = 1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c875dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "X,Y=sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "005dd89d-0ee0-497a-9505-44c09feb3da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_noheader=np.delete(dataset,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9419d4e-ca21-4bb0-8261-65633352d03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1', ..., 3, 3, 3], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_noheader[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c00dd21c-a4c5-4b9a-8161-e518270f92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_noheader[:, 0:4].astype(float)\n",
    "Y = dataset_noheader[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c4e19b2-f5f6-4742-b060-93f8963c6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059a957-a2b1-4641-b355-ff50189603d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e999acfa-c94c-4aa4-b3d1-d1d99a3bc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1b4cc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.216e+01 1.803e+01 7.829e+01 ... 5.690e-02 2.406e-01 7.729e-02]\n",
      " [1.553e+01 3.356e+01 1.037e+02 ... 2.014e-01 3.512e-01 1.204e-01]\n",
      " [1.831e+01 2.058e+01 1.208e+02 ... 1.510e-01 3.074e-01 7.863e-02]\n",
      " ...\n",
      " [1.227e+01 2.997e+01 7.742e+01 ... 0.000e+00 2.409e-01 6.743e-02]\n",
      " [1.283e+01 1.573e+01 8.289e+01 ... 9.783e-02 3.006e-01 7.802e-02]\n",
      " [1.398e+01 1.962e+01 9.112e+01 ... 1.827e-01 3.179e-01 1.055e-01]]\n",
      "[1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0\n",
      " 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
      " 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
      " 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
      " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
      " 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "624a3521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 358)\n",
      "(30, 40)\n",
      "(171, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b55db06e-7ef3-43d6-86aa-1ad7d07604a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "scaler=sk.preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5ef9fe2b-fcb7-4ca0-8b2d-508bd35f9180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "for a in range(X_train.shape[1]):\n",
    "  X_train[:,a]=scaler.fit_transform(X_train[:,a].reshape(-1, 1)).flatten()\n",
    "  print(a)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28f23a-7a0b-46f1-b031-a6273d1c4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_train)):\n",
    "    print(Y_train[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fdb103ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np.array(pd.get_dummies(np.array(Y_train)))\n",
    "Y_dev=np.array(pd.get_dummies(np.array(Y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b357075e-5e92-4ceb-aa1b-9ce8844dbb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358, 2), (40, 2))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape,Y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e6410867-9cbd-4306-a2c2-34f6c152dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09e16759",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.transpose(X_train)\n",
    "X_dev=np.transpose(X_dev)\n",
    "Y_train=np.transpose(Y_train)\n",
    "Y_dev=np.transpose(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4920244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 358), (2, 358))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f4ea728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numHidden1 = 5 # number of hidden nodes\n",
    "numHidden2 = 5# number of hidden nodes\n",
    "numHidden3 = 5# number of hidden nodes\n",
    "numHidden4 = 5# number of hidden nodes\n",
    "numHidden5 = 5# number of hidden nodes\n",
    "numHidden6 = 5# number of hidden nodes\n",
    "num_features = X_train.shape[0]\n",
    "numOutput = 2\n",
    "max_epoches = 1000000\n",
    "alpha = 0.001\n",
    "epsilon=0.0000001\n",
    "NN = NeuralNet(num_features, numHidden1,numHidden2,numHidden3,numHidden4,numHidden5,numHidden6, alpha, max_epoches, numOutput,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3e4141b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bias_H1_to_H2',\n",
       " 'Bias_H2_to_H3',\n",
       " 'Bias_H3_to_H4',\n",
       " 'Bias_H4_to_H5',\n",
       " 'Bias_H5_to_H6',\n",
       " 'Bias_H6_to_output',\n",
       " 'Bias_Input_to_H1',\n",
       " 'Softmax_grad',\n",
       " 'Weights_H1_to_H2',\n",
       " 'Weights_H2_to_H3',\n",
       " 'Weights_H3_to_H4',\n",
       " 'Weights_H4_to_H5',\n",
       " 'Weights_H5_to_H6',\n",
       " 'Weights_H6_to_output',\n",
       " 'Weights_Input_to_H1',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'alpha',\n",
       " 'backprop',\n",
       " 'dBias_H1_to_H2',\n",
       " 'dBias_H2_to_H3',\n",
       " 'dBias_H3_to_H4',\n",
       " 'dBias_H4_to_H5',\n",
       " 'dBias_H5_to_H6',\n",
       " 'dBias_H6_to_output',\n",
       " 'dBias_Input_to_H1',\n",
       " 'dWeights_H1_to_H2',\n",
       " 'dWeights_H2_to_H3',\n",
       " 'dWeights_H3_to_H4',\n",
       " 'dWeights_H4_to_H5',\n",
       " 'dWeights_H5_to_H6',\n",
       " 'dWeights_H6_to_output',\n",
       " 'dWeights_Input_to_H1',\n",
       " 'deriv',\n",
       " 'derivtanh',\n",
       " 'fit',\n",
       " 'forward',\n",
       " 'loss',\n",
       " 'max_epochs',\n",
       " 'num_features',\n",
       " 'num_hidden1',\n",
       " 'num_hidden2',\n",
       " 'num_hidden3',\n",
       " 'num_hidden4',\n",
       " 'num_hidden5',\n",
       " 'num_hidden6',\n",
       " 'num_output',\n",
       " 'predict',\n",
       " 'relU',\n",
       " 'sigmoid']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "abf6e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = {(\"NN.\"+attr) for attr in dir(NN) if not callable(getattr(NN, attr)) and not attr.startswith(\"__\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a03b6454-5978-4264-8f94-f2eb4b851b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN.Bias_H1_to_H2',\n",
       " 'NN.Bias_H2_to_H3',\n",
       " 'NN.Bias_H3_to_H4',\n",
       " 'NN.Bias_H4_to_H5',\n",
       " 'NN.Bias_H5_to_H6',\n",
       " 'NN.Bias_H6_to_output',\n",
       " 'NN.Bias_Input_to_H1',\n",
       " 'NN.CCloss',\n",
       " 'NN.Weights_H1_to_H2',\n",
       " 'NN.Weights_H2_to_H3',\n",
       " 'NN.Weights_H3_to_H4',\n",
       " 'NN.Weights_H4_to_H5',\n",
       " 'NN.Weights_H5_to_H6',\n",
       " 'NN.Weights_H6_to_output',\n",
       " 'NN.Weights_Input_to_H1',\n",
       " 'NN.a1',\n",
       " 'NN.a2',\n",
       " 'NN.a3',\n",
       " 'NN.a4',\n",
       " 'NN.a5',\n",
       " 'NN.a6',\n",
       " 'NN.a7',\n",
       " 'NN.alpha',\n",
       " 'NN.dBias_H1_to_H2',\n",
       " 'NN.dBias_H2_to_H3',\n",
       " 'NN.dBias_H3_to_H4',\n",
       " 'NN.dBias_H4_to_H5',\n",
       " 'NN.dBias_H5_to_H6',\n",
       " 'NN.dBias_H6_to_output',\n",
       " 'NN.dBias_Input_to_H1',\n",
       " 'NN.dWeights_H1_to_H2',\n",
       " 'NN.dWeights_H2_to_H3',\n",
       " 'NN.dWeights_H3_to_H4',\n",
       " 'NN.dWeights_H4_to_H5',\n",
       " 'NN.dWeights_H5_to_H6',\n",
       " 'NN.dWeights_H6_to_output',\n",
       " 'NN.dWeights_Input_to_H1',\n",
       " 'NN.dz1',\n",
       " 'NN.dz2',\n",
       " 'NN.dz3',\n",
       " 'NN.dz4',\n",
       " 'NN.dz5',\n",
       " 'NN.dz6',\n",
       " 'NN.dz7',\n",
       " 'NN.loss',\n",
       " 'NN.max_epochs',\n",
       " 'NN.num_features',\n",
       " 'NN.num_hidden1',\n",
       " 'NN.num_hidden2',\n",
       " 'NN.num_hidden3',\n",
       " 'NN.num_hidden4',\n",
       " 'NN.num_hidden5',\n",
       " 'NN.num_hidden6',\n",
       " 'NN.num_output',\n",
       " 'NN.z1',\n",
       " 'NN.z2',\n",
       " 'NN.z3',\n",
       " 'NN.z4',\n",
       " 'NN.z5',\n",
       " 'NN.z6',\n",
       " 'NN.z7'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "940a896a-cc24-448d-a761-7ecef9b66144",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37130e1e-df01-42e5-889d-5bf7558d7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in members:\n",
    "    try:\n",
    "        dic[i]=eval(i).shape\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79977f93-2d57-4b0e-a56a-871d60c01e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN.z1': (500, 358),\n",
       " 'NN.Weights_H5_to_H6': (500, 500),\n",
       " 'NN.a5': (500, 358),\n",
       " 'NN.Bias_H4_to_H5': (500, 1),\n",
       " 'NN.Weights_H6_to_output': (2, 500),\n",
       " 'NN.Bias_Input_to_H1': (500, 1),\n",
       " 'NN.a1': (500, 358),\n",
       " 'NN.z4': (500, 358),\n",
       " 'NN.Bias_H1_to_H2': (500, 1),\n",
       " 'NN.Bias_H6_to_output': (2, 1),\n",
       " 'NN.dWeights_H2_to_H3': (500, 500),\n",
       " 'NN.dBias_H2_to_H3': (500, 1),\n",
       " 'NN.dBias_H5_to_H6': (500, 1),\n",
       " 'NN.Weights_H4_to_H5': (500, 500),\n",
       " 'NN.a4': (500, 358),\n",
       " 'NN.dWeights_H4_to_H5': (500, 500),\n",
       " 'NN.dWeights_Input_to_H1': (500, 30),\n",
       " 'NN.dBias_H6_to_output': (2, 1),\n",
       " 'NN.z6': (500, 358),\n",
       " 'NN.dWeights_H5_to_H6': (500, 500),\n",
       " 'NN.Bias_H2_to_H3': (500, 1),\n",
       " 'NN.dz4': (500, 358),\n",
       " 'NN.dBias_H4_to_H5': (500, 1),\n",
       " 'NN.dz7': (2, 358),\n",
       " 'NN.dBias_Input_to_H1': (500, 1),\n",
       " 'NN.dz3': (500, 358),\n",
       " 'NN.Bias_H3_to_H4': (500, 1),\n",
       " 'NN.Weights_Input_to_H1': (500, 30),\n",
       " 'NN.a3': (500, 358),\n",
       " 'NN.CCloss': (),\n",
       " 'NN.dWeights_H3_to_H4': (500, 500),\n",
       " 'NN.dz1': (500, 358),\n",
       " 'NN.dz5': (500, 358),\n",
       " 'NN.dz6': (500, 358),\n",
       " 'NN.z3': (500, 358),\n",
       " 'NN.Weights_H2_to_H3': (500, 500),\n",
       " 'NN.a6': (500, 358),\n",
       " 'NN.a7': (2, 358),\n",
       " 'NN.dBias_H3_to_H4': (500, 1),\n",
       " 'NN.z2': (500, 358),\n",
       " 'NN.Bias_H5_to_H6': (500, 1),\n",
       " 'NN.Weights_H1_to_H2': (500, 500),\n",
       " 'NN.z7': (2, 358),\n",
       " 'NN.dWeights_H1_to_H2': (500, 500),\n",
       " 'NN.dz2': (500, 358),\n",
       " 'NN.Weights_H3_to_H4': (500, 500),\n",
       " 'NN.dWeights_H6_to_output': (2, 500),\n",
       " 'NN.dBias_H1_to_H2': (500, 1),\n",
       " 'NN.z5': (500, 358),\n",
       " 'NN.a2': (500, 358)}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f0c718e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1055.4053010781367,\n",
       " 990.4685312623294,\n",
       " 984.0906535506243,\n",
       " 981.0977661982902,\n",
       " 979.2719603906143,\n",
       " 977.9604668079212,\n",
       " 977.9882497007924,\n",
       " 976.6713745731705,\n",
       " 975.7775211123733,\n",
       " 975.2059001425464,\n",
       " 974.580739965045,\n",
       " 974.1954906583907,\n",
       " 973.8000308675845,\n",
       " 973.4658598569606,\n",
       " 973.1750348610234,\n",
       " 972.9290660288522,\n",
       " 972.7693870752452,\n",
       " 972.5861663260093,\n",
       " 972.403278149314,\n",
       " 972.2656055783361,\n",
       " 972.1432207964966,\n",
       " 971.9738032419286,\n",
       " 971.7965080002409,\n",
       " 971.7351354168084,\n",
       " 971.4995557670079,\n",
       " 971.4366846766904,\n",
       " 971.397888978711,\n",
       " 971.3753623393108,\n",
       " 971.3162796439331,\n",
       " 971.2074218442012,\n",
       " 971.1533652431594,\n",
       " 971.0249696956846,\n",
       " 971.0162434771096,\n",
       " 970.9948693516047,\n",
       " 970.868063480127,\n",
       " 970.7120740152661,\n",
       " 970.6188204988991,\n",
       " 970.6068821462569,\n",
       " 970.5308979337352,\n",
       " 970.5518828292941,\n",
       " 970.3999471478885,\n",
       " 970.2277444536257,\n",
       " 970.2914634958385,\n",
       " 970.1286894106241,\n",
       " 970.1011253900341,\n",
       " 970.0110455195754,\n",
       " 969.9654457816071,\n",
       " 970.0014777969202,\n",
       " 969.812554869546,\n",
       " 969.8420926994218,\n",
       " 969.7154901156521,\n",
       " 969.6828308400221,\n",
       " 969.651191170289,\n",
       " 969.5936894116585,\n",
       " 969.5208360456597,\n",
       " 969.4778716408786,\n",
       " 969.4256979357167,\n",
       " 969.3874063032508,\n",
       " 969.3238033242494,\n",
       " 969.2187596670564,\n",
       " 969.2373120482961,\n",
       " 969.1827694628785,\n",
       " 969.1222690334394,\n",
       " 969.0764686415237,\n",
       " 969.0579621162756,\n",
       " 968.9630569358084,\n",
       " 968.9351215678407,\n",
       " 968.9116896210262,\n",
       " 968.8915721916906,\n",
       " 968.8222404859885,\n",
       " 968.7640345597749,\n",
       " 968.7700883677157,\n",
       " 968.762131028875,\n",
       " 968.7904601827281,\n",
       " 968.7523828151088,\n",
       " 968.6734081110651,\n",
       " 968.6798413696521,\n",
       " 968.6189436046504,\n",
       " 968.5439670070405,\n",
       " 968.4763808564353,\n",
       " 968.5137668808013,\n",
       " 968.4034902547528,\n",
       " 968.3166227089035,\n",
       " 968.3231239627007,\n",
       " 968.2125472679872,\n",
       " 968.2381847981849,\n",
       " 968.2263757177072,\n",
       " 968.1793564870641,\n",
       " 968.1405583283577,\n",
       " 968.211187984836,\n",
       " 968.1141623072447,\n",
       " 968.0451070347552,\n",
       " 967.9203780777549,\n",
       " 967.9145888457571,\n",
       " 967.9499668262181,\n",
       " 967.9469369031958,\n",
       " 967.9213842700287,\n",
       " 967.9190185749052,\n",
       " 967.850994768241,\n",
       " 967.8191695757633,\n",
       " 967.8114647504265,\n",
       " 967.7323233392606,\n",
       " 967.7714888117748,\n",
       " 967.7809449341295,\n",
       " 967.6081595288663,\n",
       " 967.6342385875776,\n",
       " 967.5875485619641,\n",
       " 967.4930829482147,\n",
       " 967.4809245653955,\n",
       " 967.4509085763216,\n",
       " 967.5010642173711,\n",
       " 967.4970660240723,\n",
       " 967.4553190002953,\n",
       " 967.3933324937177,\n",
       " 967.3909737960528,\n",
       " 967.3465881713603,\n",
       " 967.4133240864892,\n",
       " 967.327386291056,\n",
       " 967.2571895347057,\n",
       " 967.1926113627198,\n",
       " 967.1572637624804,\n",
       " 967.1345376857488,\n",
       " 967.1451612519597,\n",
       " 967.1167273794583,\n",
       " 967.0223217794102,\n",
       " 967.0743753276427,\n",
       " 967.0436718425905,\n",
       " 966.9912865708679,\n",
       " 966.9722091034539,\n",
       " 966.9373959344161,\n",
       " 966.919582444023,\n",
       " 966.9339335958998,\n",
       " 966.9743075308609,\n",
       " 967.0023156313562,\n",
       " 966.9973650512094,\n",
       " 966.906761081929,\n",
       " 966.7676743592908,\n",
       " 966.706391355717,\n",
       " 966.6440070006139,\n",
       " 966.6413231484634,\n",
       " 966.6660270202418]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84ae73a8-3f40-4bba-8d18-12c3b5837711",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in dic.items():\n",
    "    try:\n",
    "        print(key,value.shape)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c4522b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 122553)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.forward(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9937771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(NN.Weights_H6_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.dot(NN.dy,NN.Weights_H6_to_output)*(NN.deriv(NN.z6))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d1abd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.backprop(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c144d148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 80)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.z7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d0b12d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 122553)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X_train.T,NN.dz1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "aba54ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "13c0bcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.31224095e-08 2.31967200e-06 9.99888522e-01 5.93015539e-02\n",
      "  9.99999937e-01 9.99833386e-01 1.48396418e-02 5.71529441e-05\n",
      "  9.99992210e-01 5.50128327e-04 1.42403625e-01 1.66198914e-05\n",
      "  9.99999995e-01 9.99999995e-01 6.44264144e-09 1.29044725e-01\n",
      "  9.99995945e-01 1.81294141e-04 1.66723745e-02 9.99985941e-01\n",
      "  1.04521375e-02 9.99999959e-01 9.99992584e-01 9.99945107e-01\n",
      "  2.72418376e-02 4.29370957e-05 3.15047909e-02 9.99999026e-01\n",
      "  7.94003347e-04 9.99999058e-01 9.99999062e-01 9.99888522e-01\n",
      "  9.99996220e-01 9.99985338e-01 4.65804936e-07 1.42773619e-05\n",
      "  6.29374828e-06 9.99998127e-01 9.99992584e-01 9.99999961e-01\n",
      "  9.99999026e-01 1.00000000e+00 6.58410938e-05 6.29374828e-06\n",
      "  9.99992471e-01 7.10075645e-03 8.16678601e-07 9.99996245e-01\n",
      "  9.99998127e-01 9.99833386e-01 1.00000000e+00 9.99999816e-01\n",
      "  9.99999472e-01 2.25885462e-03 3.06674374e-04 1.94657225e-05\n",
      "  9.99999995e-01 9.99999959e-01 8.26793802e-03 3.48325713e-10\n",
      "  1.22956840e-01 1.81294141e-04 3.31649220e-02 1.04521375e-02\n",
      "  2.85816522e-05 2.47240665e-01 6.93790843e-06 9.21648018e-06\n",
      "  1.45475272e-02 9.99986461e-01 1.00000000e+00 9.60561680e-01\n",
      "  9.99374381e-01 9.99984344e-01 1.06295276e-05 2.70198771e-06\n",
      "  2.88115519e-03 9.15236639e-11 9.99999754e-01 9.99942595e-01]\n",
      " [9.99999987e-01 9.99997680e-01 1.11477566e-04 9.40698446e-01\n",
      "  6.29104970e-08 1.66614338e-04 9.85160358e-01 9.99942847e-01\n",
      "  7.79011727e-06 9.99449872e-01 8.57596375e-01 9.99983380e-01\n",
      "  4.64193902e-09 4.64193902e-09 9.99999994e-01 8.70955275e-01\n",
      "  4.05540573e-06 9.99818706e-01 9.83327626e-01 1.40587475e-05\n",
      "  9.89547862e-01 4.06166191e-08 7.41635797e-06 5.48933805e-05\n",
      "  9.72758162e-01 9.99957063e-01 9.68495209e-01 9.73510068e-07\n",
      "  9.99205997e-01 9.42395076e-07 9.37699156e-07 1.11477566e-04\n",
      "  3.78042754e-06 1.46624575e-05 9.99999534e-01 9.99985723e-01\n",
      "  9.99993706e-01 1.87282915e-06 7.41635797e-06 3.94628784e-08\n",
      "  9.73510068e-07 8.52559375e-12 9.99934159e-01 9.99993706e-01\n",
      "  7.52917887e-06 9.92899244e-01 9.99999183e-01 3.75463825e-06\n",
      "  1.87282915e-06 1.66614338e-04 7.19078389e-12 1.84055191e-07\n",
      "  5.28482435e-07 9.97741145e-01 9.99693326e-01 9.99980534e-01\n",
      "  4.64193902e-09 4.06166191e-08 9.91732062e-01 1.00000000e+00\n",
      "  8.77043160e-01 9.99818706e-01 9.66835078e-01 9.89547862e-01\n",
      "  9.99971418e-01 7.52759335e-01 9.99993062e-01 9.99990784e-01\n",
      "  9.85452473e-01 1.35387997e-05 7.32549210e-11 3.94383203e-02\n",
      "  6.25618822e-04 1.56562756e-05 9.99989370e-01 9.99997298e-01\n",
      "  9.97118845e-01 1.00000000e+00 2.45634603e-07 5.74053765e-05]]\n"
     ]
    }
   ],
   "source": [
    "print (NN.a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfde5919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(NN.a7,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bca0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7214af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 176400,  loss: 30.645707296027708821384294424206018447875976562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 176500,  loss: 30.644922842288007558408935437910258769989013671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 176600,  loss: 30.644138271004354834303740062750875949859619140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 176700,  loss: 30.643353582843118942946603056043386459350585937500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 176800,  loss: 30.642568776783505768435134086757898330688476562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 176900,  loss: 30.641783853585334185254396288655698299407958984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177000,  loss: 30.640998812195466172170199570246040821075439453125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177100,  loss: 30.640213653346943800670487689785659313201904296875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177200,  loss: 30.639428376073784221489404444582760334014892578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177300,  loss: 30.638643161312955953690106980502605438232421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177400,  loss: 30.637857855062275547197714331559836864471435546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177500,  loss: 30.637072434800771247864759061485528945922851562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177600,  loss: 30.636286896043252170329651562497019767761230468750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177700,  loss: 30.635501238264382095621840562671422958374023437500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177800,  loss: 30.634715461702441530178475659340620040893554687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 177900,  loss: 30.633929565827713759063044562935829162597656250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178000,  loss: 30.633143550884884831475574173964560031890869140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178100,  loss: 30.632357574605475036833013291470706462860107421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178200,  loss: 30.631571509707153921908684424124658107757568359375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178300,  loss: 30.630785325037479083221114706248044967651367187500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178400,  loss: 30.629999020453457347912262775935232639312744140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178500,  loss: 30.629212595811928565581183647736907005310058593750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178600,  loss: 30.628426050970308125442898017354309558868408203125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178700,  loss: 30.627639385785837333742165355943143367767333984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178800,  loss: 30.626852600116187375078879995271563529968261718750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 178900,  loss: 30.626065693824035207626366172917187213897705078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179000,  loss: 30.625278686882914769284980138763785362243652343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179100,  loss: 30.624491518657855237961484817788004875183105468750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179200,  loss: 30.623704249626445772491933894343674182891845703125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179300,  loss: 30.622916859399982314471344579942524433135986328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179400,  loss: 30.622129347837038437774026533588767051696777343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179500,  loss: 30.621341714796418642663411446847021579742431640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179600,  loss: 30.620553960136906113120858208276331424713134765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179700,  loss: 30.619766085849072112523572286590933799743652343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179800,  loss: 30.618978085289057844420312903821468353271484375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 179900,  loss: 30.618189964927918111925464472733438014984130859375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180000,  loss: 30.617401722384993689729526522569358348846435546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180100,  loss: 30.616613357519867122391588054597377777099609375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180200,  loss: 30.615824870208502517243687179870903491973876953125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180300,  loss: 30.615036260160721326428756583482027053833007812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180400,  loss: 30.614247527488963385167153319343924522399902343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180500,  loss: 30.613458671935262600527494214475154876708984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 180600,  loss: 30.612669693360331280018726829439401626586914062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180700,  loss: 30.611880591527821593444969039410352706909179687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180800,  loss: 30.611091366491759657719740062020719051361083984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 180900,  loss: 30.610302018016565028801778680644929409027099609375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181000,  loss: 30.609512546021338863511118688620626926422119140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181100,  loss: 30.608722950101938664602130302228033542633056640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181200,  loss: 30.607933230476280783705078647471964359283447265625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181300,  loss: 30.607143386857234901299307239241898059844970703125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181400,  loss: 30.606353419018308414933926542289555072784423828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181500,  loss: 30.605563326995820716547314077615737915039062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181600,  loss: 30.604773110573635364062283770181238651275634765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181700,  loss: 30.603982769505602590243142913095653057098388671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181800,  loss: 30.603192303844078736574374488554894924163818359375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 181900,  loss: 30.602401713283398976273019798099994659423828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182000,  loss: 30.601610997840808181535976473242044448852539062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182100,  loss: 30.600820208278641132437769556418061256408691406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182200,  loss: 30.600029463224380776864563813433051109313964843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182300,  loss: 30.599238689243399846873217029497027397155761718750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182400,  loss: 30.598447789690577991450481931678950786590576171875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182500,  loss: 30.597656764427746622914128238335251808166503906250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182600,  loss: 30.596865613316733600868246867321431636810302734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182700,  loss: 30.596074336219711398143772385083138942718505859375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182800,  loss: 30.595282932999065650392367388121783733367919921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 182900,  loss: 30.594491403517466210359998513013124465942382812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183000,  loss: 30.593699747637685959489317610859870910644531250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183100,  loss: 30.592907965222867261445571784861385822296142578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183200,  loss: 30.592116056136120505470898933708667755126953125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183300,  loss: 30.591324020241060566149826627224683761596679687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183400,  loss: 30.590531857401302318066882435232400894165039062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183500,  loss: 30.589739567480712878477788763120770454406738281250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183600,  loss: 30.588947150343344105749565642327070236206054687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183700,  loss: 30.588154605853514311775143141858279705047607421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183800,  loss: 30.587361933875513386738020926713943481445312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 183900,  loss: 30.586569134274100179027300328016281127929687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184000,  loss: 30.585776206914044195173119078390300273895263671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184100,  loss: 30.584983151660292577389554935507476329803466796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184200,  loss: 30.584189968377959445433589280582964420318603515625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184300,  loss: 30.583396656932514190430083544924855232238769531250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184400,  loss: 30.582603217189262778674674336798489093780517578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184500,  loss: 30.581809649013997898236993933096528053283691406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184600,  loss: 30.581015952272508684472995810210704803466796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184700,  loss: 30.580222126830779671990967472083866596221923828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 184800,  loss: 30.579428320631912185945111559703946113586425781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 184900,  loss: 30.578634405182402389300477807410061359405517578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185000,  loss: 30.577840360632695393405811046250164508819580078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185100,  loss: 30.577046186848239273103899904526770114898681640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185200,  loss: 30.576251883694762767618158250115811824798583984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185300,  loss: 30.575457451038133172005473170429468154907226562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185400,  loss: 30.574662888744494892989678191952407360076904296875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185500,  loss: 30.573868196680212605542692472226917743682861328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185600,  loss: 30.573073374711853489316126797348260879516601562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185700,  loss: 30.572278422706268941055895993486046791076660156250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185800,  loss: 30.571483340530367200926775694824755191802978515625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 185900,  loss: 30.570688128051362042469918378628790378570556640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186000,  loss: 30.569892785136655533051452948711812496185302734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186100,  loss: 30.569097311653820270294090732932090759277343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186200,  loss: 30.568301707470610040218161884695291519165039062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186300,  loss: 30.567505972454920737391148577444255352020263671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186400,  loss: 30.566710106474936026188515825197100639343261718750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186500,  loss: 30.565914109398843123699407442472875118255615234375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186600,  loss: 30.565117981095202281949241296388208866119384765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186700,  loss: 30.564321721432619938241259660571813583374023437500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186800,  loss: 30.563525330279958325263578444719314575195312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 186900,  loss: 30.562728807506040595853846753016114234924316406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187000,  loss: 30.561932152980102017636454547755420207977294921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187100,  loss: 30.561135366571306803962215781211853027343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187200,  loss: 30.560338448149234835682364064268767833709716796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187300,  loss: 30.559541397583387833947199396789073944091796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187400,  loss: 30.558744214743590816851792624220252037048339843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187500,  loss: 30.557946899499704329628002597019076347351074218750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187600,  loss: 30.557149451721699051631730981171131134033203125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187700,  loss: 30.556351871279865406449971487745642662048339843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187800,  loss: 30.555554158044525792092827032320201396942138671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 187900,  loss: 30.554756311886194453109055757522583007812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188000,  loss: 30.553958332675392739474773406982421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188100,  loss: 30.553160220283004377961333375424146652221679687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188200,  loss: 30.552361974579927306194804259575903415679931640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188300,  loss: 30.551563595437254861053588683716952800750732421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188400,  loss: 30.550765082726062615847695269621908664703369140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188500,  loss: 30.549966436317820495105479494668543338775634765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188600,  loss: 30.549167656083916710940684424713253974914550781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188700,  loss: 30.548368741895984612710890360176563262939453125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188800,  loss: 30.547569693625789000179793220013380050659179687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 188900,  loss: 30.546770511145204807235131738707423210144042968750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 189000,  loss: 30.545971194326156705756147857755422592163085937500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189100,  loss: 30.545171743040949507985715172253549098968505859375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189200,  loss: 30.544372157161770786615306860767304897308349609375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189300,  loss: 30.543572436561010619016087730415165424346923828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189400,  loss: 30.542772581111307772516738623380661010742187500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189500,  loss: 30.541972590685251276454437174834311008453369140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189600,  loss: 30.541172465155671744696519454009830951690673828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189700,  loss: 30.540372204395499267093327944166958332061767578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189800,  loss: 30.539571808277834463751787552610039710998535156250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 189900,  loss: 30.538771276675792165633538388647139072418212890625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190000,  loss: 30.537970609462817606072349008172750473022460937500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190100,  loss: 30.537169806512181935431726742535829544067382812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190200,  loss: 30.536368867697660789417568594217300415039062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190300,  loss: 30.535567792892770455637219129130244255065917968750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190400,  loss: 30.534766581971517496185697382315993309020996093750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190500,  loss: 30.533965234807709521192009560763835906982421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190600,  loss: 30.533163751275498754012005520053207874298095703125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190700,  loss: 30.532362131249008996292104711756110191345214843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190800,  loss: 30.531560374602655372200388228520750999450683593750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 190900,  loss: 30.530758481210899191182761569507420063018798828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191000,  loss: 30.529956450948255053390312241390347480773925781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191100,  loss: 30.529154283689411641944388975389301776885986328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191200,  loss: 30.528351979309274355500747333280742168426513671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191300,  loss: 30.527549537682730829146748874336481094360351562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191400,  loss: 30.526746958684849886367373983375728130340576171875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191500,  loss: 30.525944242190849564622112666256725788116455078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191600,  loss: 30.525141388075955006797812529839575290679931640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191700,  loss: 30.524338396215686231016661622561514377593994140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191800,  loss: 30.523535266485616546106030000373721122741699218750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 191900,  loss: 30.522731998761305050038572517223656177520751953125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192000,  loss: 30.521928592918641243159072473645210266113281250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192100,  loss: 30.521125048833464887820809963159263134002685546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192200,  loss: 30.520321366381867989048259914852678775787353515625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192300,  loss: 30.519517545439974526289006462320685386657714843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192400,  loss: 30.518713585884082561960894963704049587249755859375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192500,  loss: 30.517909487590550554614310385659337043762207031250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192600,  loss: 30.517105250435843544210001709870994091033935546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192700,  loss: 30.516300874296710787803021958097815513610839843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192800,  loss: 30.515496359049777197469666134566068649291992187500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 192900,  loss: 30.514691704571969665948927286081016063690185546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193000,  loss: 30.513886910740232849548192461952567100524902343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193100,  loss: 30.513081977431671276690394734032452106475830078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 193200,  loss: 30.512276904523528031631940393708646297454833984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193300,  loss: 30.511471691893099489334417739883065223693847656250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193400,  loss: 30.510666339417774395315063884481787681579589843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193500,  loss: 30.509860846975250581181171583011746406555175781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193600,  loss: 30.509055214443083769992881570942699909210205078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193700,  loss: 30.508249441699106796477281022816896438598632812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193800,  loss: 30.507443528621227102348711923696100711822509765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 193900,  loss: 30.506637475087433841736128670163452625274658203125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194000,  loss: 30.505831280975943542443928890861570835113525390625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194100,  loss: 30.505024946164933652426043408922851085662841796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194200,  loss: 30.504218470532794782457131077535450458526611328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194300,  loss: 30.503411853958002808440141961909830570220947265625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194400,  loss: 30.502605096319133082261032541282474994659423828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194500,  loss: 30.501798197494853326361408107914030551910400390625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194600,  loss: 30.500991157364119032990856794640421867370605468750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194700,  loss: 30.500183975805747138565493514761328697204589843750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194800,  loss: 30.499376652698778400463197613134980201721191406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 194900,  loss: 30.498569187922402790036358055658638477325439453125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195000,  loss: 30.497761581355863569342545815743505954742431640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195100,  loss: 30.496953832878602952405344694852828979492187500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195200,  loss: 30.496145942370070258675696095451712608337402343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195300,  loss: 30.495337909709849810724335839040577411651611328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195400,  loss: 30.494529734777625407105006161145865917205810546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195500,  loss: 30.493721417453279798337462125346064567565917968750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195600,  loss: 30.492912957616759683787677204236388206481933593750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195700,  loss: 30.492104355148089922522558481432497501373291015625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195800,  loss: 30.491295609927366427882589050568640232086181640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 195900,  loss: 30.490486721834937355879446840845048427581787109375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196000,  loss: 30.489677690751161520665846182964742183685302734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196100,  loss: 30.488868516556511423232223023660480976104736328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196200,  loss: 30.488059199131573251406734925694763660430908203125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196300,  loss: 30.487249738357064643423655070364475250244140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196400,  loss: 30.486440134113809818927620653994381427764892578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196500,  loss: 30.485630386282654313845341675914824008941650390625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196600,  loss: 30.484820494744777619189335382543504238128662109375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196700,  loss: 30.484010459381138957724033389240503311157226562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196800,  loss: 30.483200280073088350718535366468131542205810546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 196900,  loss: 30.482389956701958055873546982184052467346191406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197000,  loss: 30.481579489149279282855786732397973537445068359375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197100,  loss: 30.480768877296473107207930297590792179107666015625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197200,  loss: 30.479958121025287454131103004328906536102294921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197300,  loss: 30.479147220217512881390575785189867019653320312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 197400,  loss: 30.478336174755089160726129193790256977081298828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197500,  loss: 30.477525028573101195661365636624395847320556640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197600,  loss: 30.476713745108323649901649332605302333831787109375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197700,  loss: 30.475902316617176524005117244087159633636474609375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197800,  loss: 30.475090742981777225395489949733018875122070312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 197900,  loss: 30.474279024084182765363948419690132141113281250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198000,  loss: 30.473467159806855164561056881211698055267333984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198100,  loss: 30.472655150032160520368051948025822639465332031250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198200,  loss: 30.471842994642667434845861862413585186004638671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198300,  loss: 30.471030693521047538752100081183016300201416015625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198400,  loss: 30.470218246550153651241998886689543724060058593750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198500,  loss: 30.469405653612788853479287354275584220886230468750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198600,  loss: 30.468592914592122156136611010879278182983398437500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198700,  loss: 30.467780029371230199330966570414602756500244140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198800,  loss: 30.466966997833317520871787564828991889953613281250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 198900,  loss: 30.466153819861915508226957172155380249023437500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199000,  loss: 30.465340495340409887603527749888598918914794921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199100,  loss: 30.464527024152374679033528082072734832763671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199200,  loss: 30.463713406181600618083393783308565616607666015625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199300,  loss: 30.462899641311892651174275670200586318969726562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199400,  loss: 30.462085729427151647996652172878384590148925781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199500,  loss: 30.461271670411417034074474941007792949676513671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199600,  loss: 30.460457464148898765188278048299252986907958984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199700,  loss: 30.459643110523735742845019558444619178771972656250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199800,  loss: 30.458828609420425692633216385729610919952392578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 199900,  loss: 30.458013960723281599030087818391621112823486328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200000,  loss: 30.457199164317053430295345606282353401184082031250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200100,  loss: 30.456384220086313519004761474207043647766113281250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200200,  loss: 30.455569127915840255127477576024830341339111328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200300,  loss: 30.454753887690518610043000080622732639312744140625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200400,  loss: 30.453938499295354347395914373919367790222167968750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200500,  loss: 30.453122962615427837818060652352869510650634765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200600,  loss: 30.452307528388629975779622327536344528198242187500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200700,  loss: 30.451492063262303844339839997701346874237060546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200800,  loss: 30.450676449432979353559858282096683979034423828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 200900,  loss: 30.449860686766950124138020328246057033538818359375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201000,  loss: 30.449044775132303897180463536642491817474365234375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201100,  loss: 30.448228714398240413174789864569902420043945312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201200,  loss: 30.447412504435487079490485484711825847625732421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201300,  loss: 30.446596145115858433882749523036181926727294921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201400,  loss: 30.445779636312465754599543288350105285644531250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201500,  loss: 30.444962977899336920017958618700504302978515625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 201600,  loss: 30.444146169751668651315412716940045356750488281250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201700,  loss: 30.443329211745343343409331282600760459899902343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201800,  loss: 30.442512103757380259594356175512075424194335937500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 201900,  loss: 30.441694845665452362482028547674417495727539062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202000,  loss: 30.440877437347957368274364853277802467346191406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202100,  loss: 30.440059878684216698729869676753878593444824218750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202200,  loss: 30.439242169553995864816897665150463581085205078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202300,  loss: 30.438424309837774472953242366202175617218017578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202400,  loss: 30.437606299416554378467481001280248165130615234375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202500,  loss: 30.436788138172030215855556889437139034271240234375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202600,  loss: 30.435969825986294523545439005829393863677978515625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202700,  loss: 30.435151362742004721440025605261325836181640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202800,  loss: 30.434332748322191264378488995134830474853515625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 202900,  loss: 30.433513982610282511132027138955891132354736328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203000,  loss: 30.432695065490335650792985688894987106323242187500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203100,  loss: 30.431875996846525112005110713653266429901123046875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203200,  loss: 30.431056776563529808754537953063845634460449218750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203300,  loss: 30.430237404526408795391034800559282302856445312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203400,  loss: 30.429417880620338365815769066102802753448486328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203500,  loss: 30.428598238087644745064608287066221237182617187500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203600,  loss: 30.427778551221461356135478126816451549530029296875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203700,  loss: 30.426958712153666652966421679593622684478759765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203800,  loss: 30.426138720766676470930178766138851642608642578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 203900,  loss: 30.425318576943492843156491289846599102020263671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204000,  loss: 30.424498280567579655553345219232141971588134765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204100,  loss: 30.423677831523036729777231812477111816406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204200,  loss: 30.422857229694162839450655155815184116363525390625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204300,  loss: 30.422036474965935326508770231157541275024414062500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204400,  loss: 30.421215567223569564703211653977632522583007812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204500,  loss: 30.420394506352852914687900920398533344268798828125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204600,  loss: 30.419573292239668660386087140068411827087402343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204700,  loss: 30.418751924770475625336985103785991668701171875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204800,  loss: 30.417930403831871188913282821886241436004638671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 204900,  loss: 30.417108729310957215830057975836098194122314453125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205000,  loss: 30.416286901094984784776897868141531944274902343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205100,  loss: 30.415464919071489191537693841382861137390136718750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205200,  loss: 30.414642783128279290849604876711964607238769531250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205300,  loss: 30.413820493153469470826166798360645771026611328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205400,  loss: 30.412998049035355307978534256108105182647705078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205500,  loss: 30.412175450662420672642838326282799243927001953125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205600,  loss: 30.411352697923486942954696132801473140716552734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205700,  loss: 30.410529790707510500169519218616187572479248046875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 205800,  loss: 30.409706728903568517807798343710601329803466796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 205900,  loss: 30.408883512401050808193758712150156497955322265625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206000,  loss: 30.408060141089528372049244353547692298889160156250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206100,  loss: 30.407236614858611289946566103026270866394042968750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206200,  loss: 30.406412933598243597543842042796313762664794921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206300,  loss: 30.405589097198436832059087464585900306701660156250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206400,  loss: 30.404765105549394377248972887173295021057128906250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206500,  loss: 30.403940958541440409135248046368360519409179687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206600,  loss: 30.403116656065073186709923902526497840881347656250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206700,  loss: 30.402292198010975710076309042051434516906738281250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206800,  loss: 30.401467584269848742906106053851544857025146484375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 206900,  loss: 30.400642814732666607824285165406763553619384765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207000,  loss: 30.399817889290464023588356212712824344635009765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207100,  loss: 30.398992807834360974084120243787765502929687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207200,  loss: 30.398167570255651526167639531195163726806640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207300,  loss: 30.397342176445903305648243986070156097412109375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207400,  loss: 30.396516626296410379381995880976319313049316406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207500,  loss: 30.395690919699074328264032374136149883270263671875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207600,  loss: 30.394865056545530279663580586202442646026611328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207700,  loss: 30.394039036727665603621062473393976688385009765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207800,  loss: 30.393212860137555964001876418478786945343017578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 207900,  loss: 30.392386526667195312256808392703533172607421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208000,  loss: 30.391560051936682640416620415635406970977783203125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208100,  loss: 30.390733564451831938413306488655507564544677734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208200,  loss: 30.389906919723561173896086984314024448394775390625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208300,  loss: 30.389080117642393474852724466472864151000976562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208400,  loss: 30.388253158100130946195349679328501224517822265625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208500,  loss: 30.387426040988614772686560172587633132934570312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208600,  loss: 30.386598806978273756840280839242041110992431640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208700,  loss: 30.385771422275180242422720766626298427581787109375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208800,  loss: 30.384943879686371559500912553630769252777099609375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 208900,  loss: 30.384116179102687027580032008700072765350341796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209000,  loss: 30.383288320415569927490651025436818599700927734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209100,  loss: 30.382460303516364064080335083417594432830810546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209200,  loss: 30.381632128296910622111681732349097728729248046875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209300,  loss: 30.380803794649246185599622549489140510559082031250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209400,  loss: 30.379975302465428654841161915101110935211181640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209500,  loss: 30.379146651637913834065329865552484989166259765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209600,  loss: 30.378317842059267661625199252739548683166503906250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209700,  loss: 30.377488873622265685980892158113420009613037109375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209800,  loss: 30.376659746219836222280719084665179252624511718750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 209900,  loss: 30.375830459744989298087602946907281875610351562500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 210000,  loss: 30.375001014091147055751207517459988594055175781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210100,  loss: 30.374171409151649925206584157422184944152832031250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210200,  loss: 30.373341644820175844188270275481045246124267578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210300,  loss: 30.372511720990271300024687661789357662200927734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210400,  loss: 30.371681637556005028955041780136525630950927734375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210500,  loss: 30.370851394411253920679882867261767387390136718750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210600,  loss: 30.370020991450225267271889606490731239318847656250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210700,  loss: 30.369190428567197415077316691167652606964111328125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210800,  loss: 30.368359705656434499587703612633049488067626953125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 210900,  loss: 30.367528822612616323795009520836174488067626953125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211000,  loss: 30.366697779330337425562902353703975677490234375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211100,  loss: 30.365866575704266949742304859682917594909667968750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211200,  loss: 30.365035211629418654410983435809612274169921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211300,  loss: 30.364203687000607345680691651068627834320068359375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211400,  loss: 30.363372001713006653744741925038397312164306640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211500,  loss: 30.362540155661875473924737889319658279418945312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211600,  loss: 30.361708148742390989127670763991773128509521484375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211700,  loss: 30.360875980849964861363332602195441722869873046875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211800,  loss: 30.360043651880186388325455482117831707000732421875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 211900,  loss: 30.359211161728531180870049865916371345520019531250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212000,  loss: 30.358378510290886964639867073856294155120849609375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212100,  loss: 30.357545697462889222606463590636849403381347656250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212200,  loss: 30.356712723140532261822954751551151275634765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212300,  loss: 30.355879587219732229641522280871868133544921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212400,  loss: 30.355046289596518960252069518901407718658447265625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212500,  loss: 30.354212830167202952225125045515596866607666015625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212600,  loss: 30.353379208827970359152459423057734966278076171875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212700,  loss: 30.352545442423668475839804159477353096008300781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212800,  loss: 30.351711639745367676823661895468831062316894531250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 212900,  loss: 30.350877674778121928511609439738094806671142578125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213000,  loss: 30.350043547417996592230338137596845626831054687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213100,  loss: 30.349209257561820862747481442056596279144287109375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213200,  loss: 30.348374805106178797586835571564733982086181640625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213300,  loss: 30.347540189948016831067434395663440227508544921875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213400,  loss: 30.346705411984274292080954182893037796020507812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213500,  loss: 30.345870471111915378514822805300354957580566406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213600,  loss: 30.345035367228099687508802162483334541320800781250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213700,  loss: 30.344200100230096950326696969568729400634765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213800,  loss: 30.343364670015073869535626727156341075897216796875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 213900,  loss: 30.342529076480552419070590985938906669616699218750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214000,  loss: 30.341693319523923122460473678074777126312255859375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214100,  loss: 30.340857399042803876909601967781782150268554687500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 214200,  loss: 30.340021314934720209066654206253588199615478515625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214300,  loss: 30.339185067097528047952437191270291805267333984375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214400,  loss: 30.338348655429015821027860511094331741333007812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214500,  loss: 30.337512079827057220882124966010451316833496093750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214600,  loss: 30.336675340189625416087437770329415798187255859375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214700,  loss: 30.335838436414842789190515759401023387908935546875000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214800,  loss: 30.335001368400774879319214960560202598571777343750000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 214900,  loss: 30.334164136045728810131549835205078125000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n",
      "step: 215000,  loss: 30.333326739247947756439316435717046260833740234375000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "0.9329608938547486\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "NN.fit(X_train,Y_train,X_dev,Y_dev)\n",
    "# predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf5e69db-1818-4849-ba7a-ab948d12da05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/06/9glg8hyx2ys8hlw5f9dql_3h0000gn/T/ipykernel_2692/512948447.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy is {:1.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15, 3]"
     ]
    }
   ],
   "source": [
    "Y_pred = NN.predict(X_test.T)\n",
    "Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "print(\"accuracy is {:1.3f}\".format(accuracy_score(Y_test.T.astype(int), Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b7a6b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.133\n"
     ]
    }
   ],
   "source": [
    "Y_pred = NN.forward(np.transpose(X_test))\n",
    "#Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "print(\"accuracy is {:1.3f}\".format(accuracy_score(np.argmax(np.transpose(Y_test).astype(int),axis=0), np.argmax(Y_pred,axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40761024",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/06/9glg8hyx2ys8hlw5f9dql_3h0000gn/T/ipykernel_5463/1093156827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy is {:1.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[1;32m     93\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and binary targets"
     ]
    }
   ],
   "source": [
    "Y_pred = NN.predict(X_train)\n",
    "Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "print(\"accuracy is {:1.3f}\".format(accuracy_score(Y_train.astype(int), Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f3708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=np.array(pd.get_dummies(np.array(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f19847",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.backprop(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c75b586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test cell ###\n",
    "np.mean(np.argmax(Y_pred,axis=0)==np.argmax(np.transpose(Y_test).astype(int),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.dWeights_H5_to_H6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm(NN.y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(NN.do*NN.Softmax_grad(NN.y),axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "59e082f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.alpha=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.y[NN.y>0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfacc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.Weights_H5_to_H6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.transpose(NN.Weights_H6_to_output),np.transpose(NN.dy)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e01fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.dy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.dz6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.z6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d77c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.Weights_H6_to_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose((np.matmul(np.transpose(NN.Weights_H5_to_H6),np.transpose(NN.dz6*NN.deriv(NN.z6))))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ddcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.dz5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cef91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
